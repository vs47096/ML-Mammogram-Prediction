import numpy
import pandas
from sklearn import tree
from sklearn import preprocessing
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

# Set to ensure same split done by train_test_split as random_state in it internally uses numpy`s random generator
numpy.random.seed(1234)

# Specified array of values which should be treated as missing or n/a values
missing_values = ['?']
# Specified names of columns to be used as features for our model
features = ['Age' , 'Shape',  'Margin',  'Density']

# Loaded data
# Here, na values list is passed to mark specified values as empty cells, so that can be removed by dropna method 
# and, headers which were missing in original file are specified by us in names parameter while loading data
data = pandas.read_csv('mammographic_masses.data', na_values=missing_values, names=['BI-RADS', 'Age' , 'Shape',  'Margin',  'Density', 'Severity'])

# Cleaning data
data.dropna(inplace=True)

#Creating array of feature (input) values & output values
data_features = data[features].values
data_output = data['Severity'].values

# Normalizing data for some of the algo which needs normalized data for example, KNN.
# Algos like decision tree, random forest which works fine with un-normalized data as well, will remain unimpacted by normalization
scaler = preprocessing.StandardScaler()
data_features = scaler.fit_transform(data_features)

(data_features_train,
 data_features_test,
 data_output_train,
 data_output_test) = train_test_split(data_features, data_output, train_size=0.75, random_state=1)

# from sklearn import preprocessing

# scaler = preprocessing.StandardScaler()
# data_features = scaler.fit_transform(data_features)

# Creating model instance & training model

# Decision tree - BLOCK START
# Declare model
# model = DecisionTreeClassifier()
# # Train model with training data
# model = model.fit(data_features_train, data_output_train)
# # Printing decision tree
# tree.plot_tree(model, feature_names=features)

# Priniting decision tree as  a proper tree image
# from IPython.display import Image  
# from six import StringIO  
# import pydotplus

# dot_data = StringIO()  
# tree.export_graphviz(model, out_file=dot_data,  
#                          feature_names=features)  
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
# Image(graph.create_png())
# Decision tree - BLOCK END

# Random forests - BLOCK START
# model = RandomForestClassifier(n_estimators=10)
# model = model.fit(data_features_train, data_output_train)
# Random forests - BLOCK END

# KNN - BLOCK START
KNN_k_optimized = 0
score_optimized = 0
for n in range(1, 50):
    model = KNeighborsClassifier(n_neighbors=n)
    scores = cross_val_score(model, data_features_train, data_output_train, cv=10)
    if scores.mean() > score_optimized:
        score_optimized = scores.mean()
        KNN_k_optimized = n
model = KNeighborsClassifier(n_neighbors=KNN_k_optimized)
model.fit(data_features_train, data_output_train)
# KNN - BLOCK END

model.score(data_features_test, data_output_test)

# We give cross_val_score a model, the entire data set and its "real" values, and the number of folds:
scores = cross_val_score(model, data_features, data_output, cv=10)

# Print the accuracy for each fold:
print(scores)

# And the mean accuracy of all 5 folds:
print(scores.mean())                
